---
title: "Heart Disease Project Kadiatou KABA"
author: "Kadiatou Kaba"
date: "12/17/2020"
output: word_document

---

```{r, Global Settings, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

```{r, Install and load the Libraries}
if (!require(ggplot2)) install.packages('ggplot2')
library(ggplot2)
if (!require(caret)) install.packages('caret')
library(caret)
if (!require(rpart.plot)) install.packages('rpart.plot')
library(rpart.plot)
if (!require(rmarkdown)) install.packages('rmarkdown')
library(rmarkdown)
if (!require(questionr)) install.packages('questionr')
library(questionr)

```


## Introduction


Ischemic heart disease (IHD) is a leading cause of death worldwide. Also referred to as coronary artery disease (CAD) and atherosclerotic cardiovascular disease (ACD), it manifests clinically as myocardial infarction and ischemic cardiomyopathy.

Coronary heart disease is often caused by the buildup of plaque, a waxy substance, inside the lining of larger coronary arteries. This buildup can partially or totally block blood flow in the large arteries of the heart. Some types of this condition may be caused by disease or injury affecting how the arteries work in the heart.

Symptoms of coronary heart disease may be different from person to person even if they have the same type of coronary heart disease. However, because many people have no symptoms, they do not know they have coronary heart disease until they have chest pain, a heart attack, or sudden cardiac arrest.

(Source: https://www.nhlbi.nih.gov/health-topics/coronary-heart-disease)


In this report, we tried to learn enough information of this topic to understand the **Heart Disease UCI dataset** and build simple models to predict whether a patient has a disease or not based on features like the heart rate during exercise or the cholesterol levels in the blood.


## Dataset
This dataset is hosted on Kaggle (Heart Disease UCI), and it was from UCI Machine Learning Repository. There are records of about 300 patients from Cleveland and the features are described in a following section.

```{r, Read the data} 
heart <- read.csv('../Project Choose Your Own/heart.csv')
```

## Cleaning the dataset

Thanks to the post of InitPic, this dataset is a bit different from the original one while the description is the same.

Part of these differences is that there were a few null values in the original dataset that have taken some values here:

A few more things to consider:

- data #93, 159, 164, 165 and 252 have ca=4 which is incorrect. In the original Cleveland dataset they are NaNs (so they should be removed)

- data #49 and 282 have thal = 0, also incorrect. They are also NaNs in the original dataset.

Because these are just a few instances, I decided to drop them.

There are also some differences regarding the features of the dataset which are corrected below.

```{r, Cleaning Heartdisease Dataset}
### Drop the null values
missing_ca_indeces <- which(heart$ca %in% 4)
missing_thal_indeces <-which(heart$thal %in% 0)
missing_values_indeces <- c(missing_ca_indeces, missing_thal_indeces)
heart <- heart[-missing_values_indeces, ]

### Transform categorical variable to R factors
heart$sex <- as.factor(heart$sex)
heart$cp <- as.factor(heart$cp)
heart$fbs <- as.factor(heart$fbs)
heart$restecg <- as.factor(heart$restecg)
heart$exang <- as.factor(heart$exang)
heart$slope <- as.factor(heart$slope)
heart$thal <- as.factor(heart$thal)
heart$target <- as.factor(heart$target)

### Changing names to the factor values for the graphs
levels(heart$sex) <- c("Female", "Male")
levels(heart$cp) <- c("Asymptomatic", "Atypical angina", "No angina", "Typical angina")
levels(heart$fbs) <- c("No", "Yes")
levels(heart$restecg) <- c("Hypertrophy", "Normal", "Abnormalities")
levels(heart$exang) <- c("No", "Yes")
levels(heart$slope) <- c("Descending", "Flat", "Ascending")
levels(heart$thal) <- c("Fixed defect", "Normal flow", "Reversible defect")
levels(heart$target) <- c("Yes", "No")

### renaming the "age" variable
heart <- rename.variable(heart, "Ã¯..age","age")
```

## Dataset features
##### _heartDisease_ Dataset

```{r, Display the Heartdisease Dataset}
head(heart)
```

## Data vizualisation

##### Target
Variable _target_: whether the patient has a heart disease or not

Value 0: yes
Value 1: no

We can see that the distribution is quite balanced. Thanks to this it wouldn't be a bad idea using accuracy to evaluate how well the models perform.

```{r, Target Bar Plot}
ggplot(heart, aes(target, fill=target)) + 
  geom_bar() +
  labs(x="Disease", y="Number of patients") +
  guides(fill=FALSE)

prop.table(table(heart$target))
```
**45.9%** of the patients in the dataset have a heart disease.

##### Age
Variable _age_: Patient age in years. In the data we can see, as expected, that age is a risk factor. In other words, the higher the age, the more likely that the patient has a heart disease.

```{r, Age histogram}
ggplot(heart, aes(age, fill=target)) + 
  geom_histogram(binwidth=1) +
  labs(fill="Disease", x="Age", y="Number of patients")

summary(heart$age)
```
The average age of the patients is **54.5**.

##### Sex
Variable _sex_: Patient's sex

Value 0: female
Value 1: male

There are approximately half the observation of women than men. We can also see that sex is a risk factor, like some of the references indicate, men are more likely to have a heart disease than women.

```{r, Sex Bar Plot}
ggplot(heart, aes(sex, fill=target)) + 
  geom_bar() +
  labs(fill="Disease", x="Sex", y="Number of patients")

prop.table(table(heart$sex))
```
**68%** of the patients are male.


##### CP
Variable _cp_: Chest pain type

Value 0: asymptomatic
Value 1: atypical angina
Value 2: pain without relation to angina
Value 3: typical angina

The description of the data doesn't provide information about how this classification of pain was made. But we can see that it is very difficult to tell whether a patient has a heart disease just looking at the symptoms of the patients.

```{r, CP Bar Plot}
ggplot(heart, aes(cp, fill=target)) +
  geom_bar() +
  labs(fill="Disease", x="Chest pain type", y="Number of patients")

prop.table(table(heart$cp))
```
**47.6%** of the patients are asymptomatic and **28%** have no angina.

##### trestbps
Variable _trestbps_: Resting blood pressure in millimeters of mercury (mm Hg) when the patient was admitted to the hospital.

By the different peaks, looks like most people tend to have a normal blood pressure inside certain groups (could be healthy adults, adults that take medication, seniors...). It also looks like very high pressures can indicate that there is a heart disease.

```{r, Trestbps histogram}
ggplot(heart, aes(trestbps, fill=target)) +
  geom_histogram(binwidth=3) +
  labs(fill="Disease", x="Blood pressure (mm Hg)", y="Number of patients")

summary(heart$trestbps)
```

##### Chol
Variable _chol_: Cholesterol level in mg/dl. This is a variable that we can control to prevent the disease. Looks like the majority of the people in the dataset have high levels of cholesterol. It also looks like up to a certain level, the presence of a heart disease is slightly higher on higher cholesterol levels. Though the cases that have the highest levels of cholesterol don't have a heart disease.

```{r, Chol histogram}
ggplot(heart, aes(chol, fill=target)) +
  geom_histogram(binwidth=10) +
  labs(fill="Disease", x="Cholesterol (mg/dl)", y="Number of patients")
```


##### Fbs
Variable _fbs_: Whether the level of sugar in the blood is higher than 120 mg/dl or not. This is another variable that we can control. However, by itself it doesn't seem very useful to know if a patient has a heart disease or not. Though we shouldn't drop it right now because it might be useful combined with other variables.

Value 0: no
Value 1: yes

```{r, Fbs Bar Plot}
ggplot(heart, aes(fbs, fill=target)) +
  geom_bar() +
  labs(fill="Disease", x="Sugar level > 120 mg/dl", y="Number of patients")
```

Hereon, variables are related to a nuclear stress test. That is, a stress test where a radioactive dye is also injected to the patient to see the blood flow.


##### Restecg
Variable _restecg_: Results of the electrocardiogram on rest

Value 0: probable left ventricular hypertrophy
Value 1: normal
Value 2: abnormalities in the T wave or ST segment
When someone has a heart disease the first symptom usually is stable angina (angina during exercise). When angina happens even on rest the disease got worse (usually due to a narrowing of the coronary arteries). This has to be why there are so few patients that show an abnormality on the heart rate on rest, and it is also why seeing this abnormality is very indicative of a presence of a heart disease.

On the other hand, the value 0, probable presence of an hypertrophy, doesn't seem to be very indicative of the presence of a heart disease by itself. It can be because this variable is not very accurate (as noted by the "probable presence").

```{r, Restecg Bar Plot}
ggplot(heart, aes(restecg, fill=target)) +
  geom_bar() +
  labs(fill="Disease", x="Electrocardiogram on rest", y="Number of patients")
```


##### Thalach
Variable _thalach_: Maximum heart rate during the stress test

At first sight, it may seem weird to see that the higher the heart rate the lower the presence of a heart disease and vice versa. However, it makes sense taking into account that the maximum healthy heart rate depends on the age (220 - age). Thus, higher rates tend to be from younger people.

```{r, Thalach histogram}
ggplot(heart, aes(thalach, fill=target)) +
  geom_histogram(binwidth=10) +
  labs(fill="Disease", x="Maximum heart rate during exercise", y="Number of patients")
```


Eight patients with a heart rate during exercise lower than 100:
```{r, heart rate < 100}
heart[heart$thalach < 100, c("age", "thalach", "target")]
```

Eighteen patients with a heart rate during exercise higher than 180:
```{r, heart rate > 180}
heart[heart$thalach > 180, c("age", "thalach", "target")]
```


##### Exang
Variable _exang_: Whether the patient had angina during exercise

Value 0: no
Value 1: yes

We can see that this feature is a good indicator for the presence of heart disease. However, we can also see that knowing what is angina and what not is not an easy task, it can be confused with other pains or it can be atypical angina.

```{r, Exang Bar Plot}
ggplot(heart, aes(exang, fill=target)) +
  geom_bar() +
  labs(fill="Disease", x="Presence of angina during exercise", y="Number of patients")
```

##### Oldpeak

Variable _oldpeak_: Decrease of the ST segment during exercise according to the same one on rest.

The ST segment is a part of the electrocardiogram of a heart beat that is usually found at a certain level in a normal heart beat. A significant displacement of this segment can indicate the presence of a heart disease as we can see in the plot.

```{r, Oldpeak histogram}
ggplot(heart, aes(oldpeak, fill=target)) +
  geom_histogram(binwidth=0.25) +
  labs(fill="Disease", x="Depression of the ST segment", y="Number of patients")
```

##### slope
Variable _slope_: Slope of the ST segment during the most demanding part of the exercise

Value 0: descending
Value 1: flat
Value 2: ascending

In the first graph, we can see that the slope by itself can help determine whether there is a heart disease or not if it is flat or ascending. However, if the slope is descending it doesn't seem to give much information. Because of this, in the second graph a third variable was added and we can notice that, if the slope is descending, the depression of the ST segment can help to determine if the patient has a heart disease.

```{r, slope Bar Plot}
ggplot(heart, aes(slope, fill=target)) +
  geom_bar() +
  labs(fill="Disease", x="Slope of the ST segment", y="Number of patients")
```

```{r, slope BoxPlot}
ggplot(heart, aes(x=slope, y=oldpeak, fill=target)) +
  geom_boxplot() +
  labs(fill="Disease", x="Slope of the ST segment", y="Depression of the ST segment")
```

##### Thal
Variable _thal_: Results of the blood flow observed via the radioactive dye.

Value 0: NULL (dropped from the dataset previously)
Value 1: fixed defect (no blood flow in some part of the heart)
Value 2: normal blood flow
Value 3: reversible defect (a blood flow is observed but it is not normal)

This feature and the next one are obtained through a very invasive process for the patients. But, by themselves, they give a very good indication of the presence of a heart disease or not.

```{r, thal Bar Plot}
ggplot(heart, aes(thal, fill=target)) +
  geom_bar() +
  labs(fill="Disease", x="Results of the blood flow", y="Number of patients")
```


##### Ca
Variable _ca_: Number of main blood vessels coloured by the radioactive dye. The number varies between 0 to 4 but the value 4 represents a null value and these have been dropped previously.

This feature refers to the number of narrow blood vessels seen, this is why the higher the value of this feature, the more likely it is to have a heart disease.

```{r, ca Bar Plot}
ggplot(heart, aes(ca, fill=target)) +
  geom_bar() +
  labs(fill="Disease", x="Number of main blood vessels coloured", y="Number of patients")
```


## Models

For this part, we chose to build four models. Three simple ones: logistic regression, naÃ¯ve bayes and decision trees. And one a bit more complex: random forest.

Null values where already dropped in a previous cell. Also, categorical variables have already been converted to R factors. Apart from this no more explicit preprocessing was done in this notebook to keep it simple and easy to follow.

To compare the models we first divide the dataset in a training set with **70%** of instances and a test set with the rest of the instances. And this taking into account that the distribution of the target has to be the same in both sets.

The test set mimics data in the real world, it will only be used at the end of the project to get a more robust measure of the models on unseen data.

The training set will be used to evaluate the models via 10 fold cross-validation. For simplicity, we'll leave the hyperparameter selection of the models by default, this means that some random combinations will be chosen and the models will be trained via cross-validation for each combination, keeping the hyperparameter combination that gave the best result.


```{r, Splitting the dataset & setting the cross-validation}
set.seed(1) # set seed to 1
training_indeces <- createDataPartition(heart$target, p = .7, list = FALSE)
heart.train <- heart[ training_indeces,]
heart.test  <- heart[-training_indeces,]

# 10 fold Cross-validation
fitControl <- trainControl(method="cv", number=10)
```

### Logistic regression

Looks like this implementation of logistic regression doesn't have any hyperparameter to tune. However, the results are not bad. A decent accuracy and a Kappa of almost 0.7 is usually considered good.

```{r, logistic regression model}
set.seed(1)
model.lr <- train(target ~ ., 
              data = heart.train,
              method = "glm",
              family=binomial(),
              trControl = fitControl)
model.lr
```
The accuracy is **0.797**.

### Decision tree

Looks like the tree was build for three different values of a cp hyperparameter. This comes from rpart and it is a hyperparameter that governs the complexity of the model: lower values give more complex (bigger) trees.

```{r, Decision model tree}
set.seed(1)
model.tree <- train(target ~ ., 
                    data = heart.train,
                    method = "rpart",
                    trControl = fitControl)
model.tree
plot(model.tree)
```

The results are not that great (**0.682**) but the tree is small and easy to interpret as we can see below.

```{r, Decision tree model final model}
rpart.plot(model.tree$finalModel)
```

### Naive Bayes

```{r, Naive Bayes model}
set.seed(1)
model.nb <- train(target ~ ., 
              data = heart.train,
              method = "naive_bayes",
              trControl = fitControl)
model.nb
```

```{r, NB model}
plot(model.nb)
```
These results are the best for now (**0.822**).

### Random Forest

```{r, Random forest model}
set.seed(1)
model.rf <- train(target ~ ., 
                  data = heart.train,
                  method = "rf",
                  trControl = fitControl)
model.rf
```

```{r, RF model}
plot(model.rf)
```
These results are also good.

## Results

### Comparison of the trainings

We can see in the following tables a summary of the results of the 10 fold cross-validation.

```{r, Results}
model.all <- list(LogisticReg=model.lr, Tree=model.tree, NaiveBayes=model.nb, RandomForest=model.rf)
results <- resamples(model.all)
summary(results, metric=c("Kappa", "Accuracy"))
```

Naive Bayes and random forest were the best models (**0.822** and **0.807** respectively). 

On the other hand, the rest of the models have metrics that vary more, this indicates that they might not be reliable to deal with unseen data.

As for final results, we'll see how Naive Bayes and Random forest perform on the test set. And we'll see which one does a good job.

## Final results

### Naive Bayes

```{r, Naive Bayes results}
preds.nb <- predict(model.nb, heart.test)
confusionMatrix(preds.nb, heart.test$target)
```

The accuracy with Naive Bayes model is **0.875**.

### Random Forest

```{r, Random Forest results}
preds.rf <- predict(model.rf, heart.test)
confusionMatrix(preds.rf, heart.test$target)
```

The accuracy with the Random Forest model is **0.841**. 

In conclusion, the _Naive Bayes_ model is the one with the highest accuracy. We also have a confusion matrix with most observations on the main diagonal.


```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```